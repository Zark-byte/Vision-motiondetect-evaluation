# Vision-MotionDetect-Evaluation

A computer vision-based 3D human motion detection and evaluation system that utilizes MediaPipe for pose estimation and RNN (Recurrent Neural Network) for intelligent motion assessment. This system can analyze human movements from standard videos and provide professional evaluation feedback.

## ‚ú® Key Features

‚Ä¢ 3D Motion Capture: Extract 3D human pose data from regular 2D videos using MediaPipe

‚Ä¢ Intelligent Motion Evaluation: Employ RNN-based deep learning models for professional movement assessment

‚Ä¢ Real-time Analysis: Capable of processing video streams in real-time or analyzing pre-recorded footage

‚Ä¢ Modular Architecture: Well-structured codebase with separate functional modules for easy customization

‚Ä¢ Comprehensive Feedback: Generate detailed evaluation reports on movement quality and form

## üõ†Ô∏è Prerequisites & Installation

Python Environment

‚Ä¢ Python 3.8 or higher

‚Ä¢ pip package manager

Install Dependencies

Install required packages
pip install -r requirements.txt


Or install manually:
Core computer vision and machine learning
pip install opencv-python mediapipe tensorflow keras numpy scipy

Data processing and visualization
pip install matplotlib pandas scikit-learn

Utilities
pip install tqdm pillow


## üöÄ Quick Start

1. Clone the Repository

git clone https://github.com/Zark-byte/Vision-MotionDetect-Evaluation.git
cd Vision-MotionDetect-Evaluation


2. Run the Complete Pipeline (Recommended for First-Time Users)

python runMain.py

This will process the example videos and demonstrate the full motion evaluation workflow.

3. Step-by-Step Execution (For Development and Customization)

Step 1: 3D Pose Capture
python 3Dcapture.py --input examples/example1.mp4 --output output_pose.npy


Step 2: Run Motion Evaluation with RNN Model
python runModule.py --pose_data output_pose.npy --model model/rnn_model.h5


Step 3: Comprehensive Analysis (Main Pipeline)
python main.py --video examples/example1.mp4 --evaluate --visualize


## üîß Usage Examples

Basic Video Analysis

python main.py --video path/to/your/video.mp4


Real-time Webcam Analysis

python main.py --webcam --real_time


Batch Processing Multiple Videos

python main.py --batch --input_dir videos/ --output_dir results/


Advanced Options

python main.py --video exercise.mp4 --confidence 0.7 --smooth_landmarks --min_detection_confidence 0.5


## üß† Technical Architecture

1. 3Dcapture.py - Pose Estimation Module

‚Ä¢ Input: Video file or real-time stream

‚Ä¢ Technology: MediaPipe Holistic for 2D-to-3D pose estimation

‚Ä¢ Output: 3D skeletal data with 33 * (x, y, z, visibility) coordinates per frame

‚Ä¢ Features: Landmark smoothing, coordinate normalization, temporal consistency

2. runModule.py - RNN Evaluation Engine

‚Ä¢ Model Type: LSTM/GRU-based recurrent neural network

‚Ä¢ Input Features: Temporal sequences of 3D pose data

‚Ä¢ Output: Motion quality scores, form assessment, error detection

‚Ä¢ Capabilities: 

  ‚Ä¢ Exercise form evaluation

  ‚Ä¢ Movement symmetry analysis

  ‚Ä¢ Range of motion assessment

  ‚Ä¢ Professional coaching feedback generation

3. main.py - Main Controller

‚Ä¢ Orchestrates the complete workflow: Capture ‚Üí Process ‚Üí Evaluate ‚Üí Visualize

‚Ä¢ Handles I/O operations and user interface

‚Ä¢ Manages configuration parameters and result presentation

## üìä Output and Results

The system generates comprehensive evaluation reports including:

‚Ä¢ Numerical Scores: Overall movement quality (0-100 scale)

‚Ä¢ Detailed Feedback: Specific form corrections and suggestions

‚Ä¢ Visual Analytics: Side-by-side comparison with ideal form

‚Ä¢ Progress Tracking: Historical performance data (when analyzing multiple sessions)

## üß™ Testing and Validation

Run the test suite to verify installation and basic functionality:
python TEST.py


For model performance testing:
python TEST.py --model_validation --test_set test_videos/


## üî¨ Customization and Extension

Adding New Exercise Models

1. Collect training data for the new exercise
2. Retrain the RNN model with additional classes
3. Update the model configuration in config/exercises.json

Modifying Evaluation Criteria

Edit the assessment parameters in the evaluation module to match specific professional standards or personal requirements.

## ü§ù Contributing

We welcome contributions! Please see our CONTRIBUTING.md for details.

1. Fork the repository
2. Create a feature branch (git checkout -b feature/AmazingFeature)
3. Commit your changes (git commit -m 'Add some AmazingFeature')
4. Push to the branch (git push origin feature/AmazingFeature)
5. Open a Pull Request

## üìû Contact and Support

‚Ä¢ Project Maintainer: Zark-byte

‚Ä¢ Email: chenkexin326@qq.com

For bug reports and feature requests, welcome to communicate with me.

## üôè Acknowledgments

‚Ä¢ https://mediapipe.dev/ for the robust pose estimation pipeline

‚Ä¢ TensorFlow/Keras team for the deep learning framework

‚Ä¢ Contributors and testers who helped improve this project
